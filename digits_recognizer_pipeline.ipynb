{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f099d0c-8322-4619-8d47-2bc0d677ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as components\n",
    "from kfp.components import OutputPath\n",
    "from kfp.components import create_component_from_func\n",
    "from kfp import dsl\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43df327f-b1cd-4862-880f-998e1541a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_batch(\n",
    "    minio_url: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    minio_data_bucket_name: str,\n",
    "    mlpipeline_ui_metadata_path: OutputPath(\"Metadata\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Get data from Minio object store\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from tensorflow import keras\n",
    "    from minio import Minio\n",
    "    from collections import namedtuple\n",
    "\n",
    "    def load_data():\n",
    "        with np.load(\"/tmp/mnist.npz\", allow_pickle=True) as f:\n",
    "            x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "            x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "    \n",
    "    # First, download the MNIST dataset locally\n",
    "    minio_client = Minio(minio_url, access_key=minio_access_key, secret_key=minio_secret_key, secure=False)\n",
    "    minio_client.fget_object(minio_data_bucket_name, \"mnist.npz\", \"/tmp/mnist.npz\")\n",
    "    \n",
    "    # Extract the MINST dataset into training and testing datasets\n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "    # Convert each set of data and save them back to Minio bucket\n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    np.save(\"/tmp/y_train.npy\", y_train)\n",
    "    np.save(\"/tmp/x_test.npy\", x_test)\n",
    "    np.save(\"/tmp/y_test.npy\", y_test)\n",
    "    minio_client.fput_object(minio_data_bucket_name, \"x_train\", \"/tmp/x_train.npy\")\n",
    "    minio_client.fput_object(minio_data_bucket_name, \"y_train\", \"/tmp/y_train.npy\")  \n",
    "    minio_client.fput_object(minio_data_bucket_name, \"x_test\", \"/tmp/x_test.npy\")    \n",
    "    minio_client.fput_object(minio_data_bucket_name, \"y_test\", \"/tmp/y_test.npy\")\n",
    "    \n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    metadata_data = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''\n",
    "**Number of training data rows:** {}; **Number of testing data rows:** {}\n",
    "'''.format(x_train.shape[0], x_test.shape[0]),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata_data, metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5732118-182a-45aa-9ae8-29086ca35ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_data(\n",
    "    minio_url: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    minio_data_bucket_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Dummy functions for showcasing\n",
    "    \"\"\"\n",
    "    print(\"Adding latest data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49139b5-3310-494d-9a56-0ff7f11b34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(\n",
    "    minio_url: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    minio_data_bucket_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Reshape the data for model building\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from minio import Minio\n",
    "    \n",
    "    minio_client = Minio(minio_url, access_key=minio_access_key, secret_key=minio_secret_key, secure=False)\n",
    "    \n",
    "    # Load data from minio\n",
    "    minio_client.fget_object(minio_data_bucket_name, \"x_train\", \"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_data_bucket_name, \"x_test\", \"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    # Reshaping the data:\n",
    "    # Reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # Normalizing the data:\n",
    "    # Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_test = x_test / 255\n",
    "    \n",
    "    # Save reshaped data from minio\n",
    "    np.save(\"/tmp/x_train.npy\", x_train)\n",
    "    minio_client.fput_object(minio_data_bucket_name, \"x_train\", \"/tmp/x_train.npy\")\n",
    "    \n",
    "    np.save(\"/tmp/x_test.npy\",x_test)\n",
    "    minio_client.fput_object(minio_data_bucket_name, \"x_test\", \"/tmp/x_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34071e9-7fd6-43b3-9bdc-cdefaeccd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(\n",
    "    num_of_epochs: int,\n",
    "    batch_size: int,\n",
    "    optimizer: str,\n",
    "    minio_url: str,\n",
    "    minio_access_key: str,\n",
    "    minio_secret_key: str,\n",
    "    minio_data_bucket_name: str,\n",
    "    run_id: str,\n",
    "    mlpipeline_ui_metadata_path: OutputPath(\"Metadata\"),\n",
    "    mlpipeline_metrics_path: OutputPath(\"Metrics\"),\n",
    "    model_output_path: OutputPath(\"Trained Model\")\n",
    ") :\n",
    "    \"\"\"\n",
    "    Build the model with Keras API. Export model parameters.\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    from minio import Minio\n",
    "    from collections import namedtuple\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import os\n",
    "    import glob\n",
    "    \n",
    "    # Load back the prepared training datasets\n",
    "    minio_client = Minio(minio_url, access_key=minio_access_key, secret_key=minio_secret_key, secure=False)\n",
    "    minio_client.fget_object(minio_data_bucket_name, \"x_train\", \"/tmp/x_train.npy\")\n",
    "    minio_client.fget_object(minio_data_bucket_name, \"y_train\", \"/tmp/y_train.npy\")\n",
    "    minio_client.fget_object(minio_data_bucket_name,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    minio_client.fget_object(minio_data_bucket_name,\"y_test\",\"/tmp/y_test.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")    \n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "    # Build an artifical neural network using Keras\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) # Output are 10 classes, numbers from 0-9\n",
    "    \n",
    "    # Compile the model - we want to have a catagorical outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model by start training and return the history while training\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=num_of_epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test, y=y_test)\n",
    "    y_pred_probs = model.predict(x=x_test)\n",
    "    y_pred_catagorical = np.argmax(y_pred_probs, axis=1) # The prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "    \n",
    "    # Save all training results as Kubeflow run metadata (shown in Visualization)\n",
    "    metadata_data = {\"outputs\": []}\n",
    "        \n",
    "    def record_model_summary(_model, _metadata_data):\n",
    "        stringlist = []\n",
    "        _model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "        metric_model_summary = \"\\n\".join(stringlist)\n",
    "        _metadata_data[\"outputs\"].append({\n",
    "            \"storage\": \"inline\",\n",
    "            \"source\": '''\n",
    "# Model Summary\n",
    "```\n",
    "{}\n",
    "```\n",
    "        '''.format(metric_model_summary),\n",
    "            'type': 'markdown'\n",
    "        })\n",
    "        \n",
    "    def record_loss_and_accuracy(_model_loss, _model_accuracy, _metadata_data):\n",
    "        _metadata_data[\"outputs\"].append({\n",
    "            \"storage\": \"inline\",\n",
    "            \"source\": '''\n",
    "# Model Performance\n",
    "- **Accuracy:** {}\n",
    "- **Loss:** {}\n",
    "        '''.format(_model_accuracy, _model_loss),\n",
    "            'type': 'markdown'\n",
    "        })\n",
    "        \n",
    "    def record_confusion_matrix(_y_test, _y_pred_catagorical, _metadata_data):\n",
    "        confusion_matrix = tf.math.confusion_matrix(labels=_y_test, predictions=_y_pred_catagorical)\n",
    "        confusion_matrix = confusion_matrix.numpy()\n",
    "        vocab = list(np.unique(_y_test))\n",
    "        data = []\n",
    "        for target_index, target_row in enumerate(confusion_matrix):\n",
    "            for predicted_index, count in enumerate(target_row):\n",
    "                data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "        _metadata_data[\"outputs\"].append({\n",
    "            \"type\": \"confusion_matrix\",\n",
    "            \"format\": \"csv\",\n",
    "            \"schema\": [\n",
    "                {'name': 'target', 'type': 'CATEGORY'},\n",
    "                {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                {'name': 'count', 'type': 'NUMBER'},\n",
    "              ],\n",
    "            \"target_col\" : \"actual\",\n",
    "            \"predicted_col\" : \"predicted\",\n",
    "            \"source\": pd.DataFrame(data, columns=['target', 'predicted', 'count']).to_csv(header=False, index=False),\n",
    "            \"storage\": \"inline\",\n",
    "            \"labels\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        })\n",
    "    \n",
    "    record_model_summary(model, metadata_data)\n",
    "    record_loss_and_accuracy(model_loss, model_accuracy, metadata_data)\n",
    "    record_confusion_matrix(y_test, y_pred_catagorical, metadata_data)\n",
    "    \n",
    "    # Result recording 1. With all the metadata, save and commit them into Kubeflow\n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata_data, metadata_file)\n",
    "    \n",
    "    # Result recording 2. Save training metrics to be shown in Kubeflow run output\n",
    "    with open(mlpipeline_metrics_path, 'w') as metrics_file:\n",
    "        json.dump({\n",
    "          'metrics': [{\n",
    "              'name': 'model_accuracy',\n",
    "              'numberValue':  float(model_accuracy),\n",
    "              'format' : \"PERCENTAGE\"\n",
    "            },{\n",
    "              'name': 'model_loss',\n",
    "              'numberValue':  float(model_loss),\n",
    "              'format' : \"PERCENTAGE\"\n",
    "            }]\n",
    "        }, metrics_file)\n",
    "        \n",
    "    # Result recording 3. Save the model to Kubeflow's artifact folder\n",
    "    keras.models.save_model(model, model_output_path)\n",
    "\n",
    "    # Finally, save all unzipped files to Minio for future uses\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    keras.models.save_model(model, \"/tmp/detect-digits\")\n",
    "    upload_local_directory_to_minio(\"/tmp/detect-digits\", minio_data_bucket_name, \"models/detect-digits/\" + run_id + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da13c87-aed3-4c3a-bac8-1807ded41b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_serving_on_kubernetes(\n",
    "    run_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    name='digits-recognizer'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"sa-minio-kserve\",\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\" + run_id + \"/\"))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    try:\n",
    "        KServe.create(isvc, namespace=namespace)\n",
    "    except:\n",
    "        print(\"The KServer service already exist, try to replace.\")\n",
    "        KServe.replace(name, isvc, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52dc383d-0c36-42fc-a648-26580b2568fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model_serving_inferernce_result():\n",
    "    \"\"\"\n",
    "    Simulate to upload an image to let the model to infer\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1SKLearnSpec\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    print(\"Actual Number: 5\")\n",
    "    x_number_five = np.array([[[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
    "             18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
    "            253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
    "            253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
    "            253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
    "            205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
    "             90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
    "            190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
    "            253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
    "            241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "             81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
    "            148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
    "            253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
    "            253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
    "            195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
    "             11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0],\n",
    "           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "              0,   0]]]])\n",
    "\n",
    "    KServe = KServeClient()\n",
    "\n",
    "    isvc_resp = KServe.get(\"digits-recognizer\", namespace=\"kubeflow-user-example-com\")\n",
    "    isvc_url = isvc_resp['status']['address']['url']\n",
    "\n",
    "    t = np.array(x_number_five)\n",
    "    t = t.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    inference_input = {\n",
    "      'instances': t.tolist()\n",
    "    }\n",
    "\n",
    "    response = requests.post(isvc_url, json=inference_input)\n",
    "    r = json.loads(response.text)\n",
    "    print(\"Predicted: {}\".format(np.argmax(r[\"predictions\"])))\n",
    "    \n",
    "    assert int(np.argmax(r[\"predictions\"])) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0667c8f9-20d9-4fe6-a8b6-c352997b4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_baseimage_url = \"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\"\n",
    "\n",
    "comp_get_data_batch = components.create_component_from_func(get_data_batch, base_image=container_baseimage_url)\n",
    "comp_get_latest_data = components.create_component_from_func(get_latest_data, base_image=container_baseimage_url)\n",
    "comp_reshape_data = components.create_component_from_func(reshape_data, base_image=container_baseimage_url)\n",
    "comp_model_building = components.create_component_from_func(model_building, base_image=container_baseimage_url)\n",
    "comp_model_serving_on_kubernetes = components.create_component_from_func(model_serving_on_kubernetes, base_image=container_baseimage_url, packages_to_install=['kserve==0.8.0.1'])\n",
    "comp_verify_model_serving_inferernce_result = components.create_component_from_func(verify_model_serving_inferernce_result, base_image=container_baseimage_url, packages_to_install=['kserve==0.8.0.1'])\n",
    "\n",
    "def disable_cache(step_list):\n",
    "    for step in step_list:\n",
    "        step.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "def digit_recognizer_pipeline(\n",
    "    num_of_epochs: int = 1,\n",
    "    batch_size: int = 20,\n",
    "    optimizer: str = 'adam',\n",
    "    minio_url: str = 'minio-service.kubeflow:9000',\n",
    "    minio_access_key: str = 'minio',\n",
    "    minio_secret_key: str = 'minio123',\n",
    "    minio_data_bucket_name: str = 'mlpipeline'\n",
    "):\n",
    "    step1_1 = comp_get_data_batch(\n",
    "        minio_url=minio_url,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key,\n",
    "        minio_data_bucket_name=minio_data_bucket_name\n",
    "    )\n",
    "    \n",
    "    step1_2 = comp_get_latest_data(\n",
    "        minio_url=minio_url,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key,\n",
    "        minio_data_bucket_name=minio_data_bucket_name\n",
    "    )\n",
    "    \n",
    "    step2 = comp_reshape_data(\n",
    "        minio_url=minio_url,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key,\n",
    "        minio_data_bucket_name=minio_data_bucket_name\n",
    "    )\n",
    "    step2.after(step1_1)\n",
    "    step2.after(step1_2)\n",
    "    \n",
    "    step3 = comp_model_building(\n",
    "        num_of_epochs=num_of_epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        minio_url=minio_url,\n",
    "        minio_access_key=minio_access_key,\n",
    "        minio_secret_key=minio_secret_key,\n",
    "        minio_data_bucket_name=minio_data_bucket_name,\n",
    "        run_id=kfp.dsl.RUN_ID_PLACEHOLDER\n",
    "    )\n",
    "    step3.after(step2)\n",
    "    \n",
    "    step4 = comp_model_serving_on_kubernetes(\n",
    "        run_id=kfp.dsl.RUN_ID_PLACEHOLDER\n",
    "    )\n",
    "    step4.after(step3)\n",
    "    \n",
    "    step5 = comp_verify_model_serving_inferernce_result()\n",
    "    step5.after(step4)\n",
    "\n",
    "    disable_cache([step1_1, step1_2, step2, step3, step4, step5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c131404c-96cc-43cf-94d7-26ad76717514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next pipeline version will be: 1.26\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please give a version description:  Fix a minor bug.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=/pipeline/#/pipelines/details/284a6348-28ec-4435-a6ec-593d48b57cf4>Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    next_version_num = \"1.\" + str((kfp.Client().list_pipeline_versions(pipeline_id='12f1e58c-a1ac-47f3-81d1-8a0ba5707cfe').total_size - 1) + 1)\n",
    "    print(\"The next pipeline version will be: \" + next_version_num)\n",
    "    next_version_description = input(\"Please give a version description: \")\n",
    "    \n",
    "    kfp.compiler.Compiler().compile(\n",
    "        pipeline_func=digit_recognizer_pipeline,\n",
    "        package_path='digit_recognizer_pipeline.yaml'\n",
    "    )\n",
    "    \n",
    "    kfp.Client().upload_pipeline_version(\n",
    "        pipeline_package_path='digit_recognizer_pipeline.yaml',\n",
    "        pipeline_version_name=next_version_num, \n",
    "        pipeline_name=\"Digit Recognizer Training and Serving Pipeline\", \n",
    "        description=next_version_description\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8aaf0-6c06-4cad-b24c-38935a86a340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
